{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 1: Extração de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Exibir versão do Python\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Checando se Scrapy está instalado\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    # URLs\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/'\n",
    "    ]\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "       \n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract(),\n",
    "                'author': quote.css('span small::text').extract(),\n",
    "                'tags': quote.css('div.tags a.tag::text').extract()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "# Iniciando processo\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 2: Gerando Arquivo de Saída"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ao executar esse trecho, reinicie o jupyter notebook"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "OBS: É importante que o kernel desse Jupyter Notebook seja restartado e reexecutado a partir daqui!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Exibir versão do Python\n",
    "import platform\n",
    "platform.python_version()\n",
    "\n",
    "try: # Checando se Scrapy está instalado\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('quoteresult.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    # URLs\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "    \n",
    "    # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'quoteresult.json'                        \n",
    "    }\n",
    "    \n",
    "    # Parse da página principal a ser crawleada\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract()[0],\n",
    "                'author': quote.css('span small::text').extract()[0],\n",
    "                'tags': quote.css('div.tags a.tag::text').extract()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "# Iniciando processo\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Carregando JSON criado para visualizar saída\n",
    "output = pd.read_json('quoteresult.jl', lines=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outro Exemplo: site SEFAZ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://normas.receita.fazenda.gov.br/sijut2consulta/consulta.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Checando se Scrapy está instalado\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # Função para gerar/abrir arquivo JSON\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('normaresult.jl', 'w')\n",
    "\n",
    "    # Fechar arquivo após escrita\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # Inserir itens coletados da página WEB no arquivo JSON criado\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SefazItem(scrapy.Item):\n",
    "    decreto = scrapy.Field()\n",
    "    publicacao_dou = scrapy.Field()\n",
    "    ementa = scrapy.Field()\n",
    "    norma = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publicacao_dou(response):\n",
    "    publicacao_dou = response.css('div.tituloPublicacao ::text')[0].extract().strip()\n",
    "        \n",
    "    return publicacao_dou\n",
    "\n",
    "\n",
    "def get_ementa(response):\n",
    "    # retrieve the ementa <div>\n",
    "    ementa_div = response.css('p.ementa::text').extract()\n",
    "\n",
    "    ementa_list = []\n",
    "    for i in ementa_div:\n",
    "        ementa_list.append(i.strip())\n",
    "\n",
    "    # that ementa_list is broken into some sentences; the function below generates the joint text\n",
    "    ementa = ' '.join(ementa_list)\n",
    "    return ementa\n",
    "\n",
    "def get_norma_elements(response):\n",
    "    # retrieve the norma <div>\n",
    "    norma = response.css('div.divSegmentos')\n",
    "\n",
    "    # declaring the list\n",
    "    norma_list = []\n",
    "    for i in norma:\n",
    "        # retrieve only the text <p> inside the <div>\n",
    "        tmp = i.css('span::text')[0].extract().strip()\n",
    "        # add each element into the norma list\n",
    "        norma_list.append(tmp)\n",
    "\n",
    "    return norma_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class NormaSpider(scrapy.Spider):\n",
    "    name = 'norma'\n",
    "    start_urls = [\n",
    "        'http://normas.receita.fazenda.gov.br/sijut2consulta/link.action?visao=anotado&idAto=95072'\n",
    "    ]\n",
    "\n",
    "    # Configuração obrigatória de pipeline para geração de arquivo de saída\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
    "        'FEED_FORMAT':'json',                                 \n",
    "        'FEED_URI': 'normaresult.json'                        \n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Output file into json of ONE single page\n",
    "        items = SefazItem()\n",
    "        items['publicacao_dou'] = get_publicacao_dou(response)\n",
    "        items['ementa'] = get_ementa(response)\n",
    "        items['norma'] = get_norma_elements(response)\n",
    "\n",
    "        yield items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "# Iniciando processo\n",
    "process.crawl(NormaSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Carregando JSON criado para visualizar saída\n",
    "output = pd.read_json('normaresult.jl', lines=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pd.options.display.max_colwidth = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links Uteis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://docs.scrapy.org/en/latest/topics/item-pipeline.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=ve_0h4Y8nuI&list=PLhTjy8cBISEqkN-5Ku_kXG4QW33sxQo0t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
